{
    "comment__": "Type of model/network to use for the actor and critic",
"model_type": "Deep_NN_Wide",
    "comment__": "Learning algorithm to use",
"agent_name": "A3C",
    "comment__": "Folder to store the training data in.",
"data_folder": "Simple_Walk_Flat",
	"comment": "initial probability of selecting a random action",
"epsilon": 0.50, 
	"comment": "initial probability of selecting a discrete random action",
"omega": 0.000,
    "comment__": "Batch size used for learning",
"batch_size": 32,
    "comment__": "Learning rate for the actor/policy",
"learning_rate": 0.001,
    "comment__": "Config file for the simulator",
"sim_config_file": "./args/genBiped2D/opt_args_imitate_biped.txt",
    "comment__": "A different config file for the forward dynamics simulator",
"forwardDynamics_config_file": "./args/genBiped2D/opt_args_imitate_biped.txt",
    "comment__": "Exploration distance use when randomly generating new actions",
"exploration_rate": 0.1,
    "comment__": "Number of rounds to perform before termination",
"rounds": 5000000,
    "comment__": "Number of epochs to perform per round",
"epochs": 10,
    "comment__": "Number of epoch/episode to evaluate the policy over",
"eval_epochs": 10,
    "comment__": "Discount factor used during learning",
"discount_factor": 0.8,
    "comment__": "Should the training be plotted during learning",
"visualize_learning": true,
    "comment__": "Whether or not to save the plotted data while learning",
"save_trainData": true,
    "comment__": "Whether or not to train a forward dynamics model as well",
"train_forward_dynamics": false,
    "comment__": "Bounds used for scaling rewards for networks",
"reward_bounds": [[-10.1],[0.0]],
    "comment__": "Max length of the Experience memory",
"expereince_length": 20000,
    "comment__": "Possible state bounds to be used for scaling states for networks",
"state_bounds": [[  7.87383467e-02,  -2.58968711e-01,   2.10417062e-02,
         -2.12261572e-01,  -2.54795939e-01,  -5.32710373e-01,
         -6.01226449e-01,  -7.81177104e-01,  -8.03433359e-01,
         -2.18906671e-01,  -2.57026047e-01,  -3.47260863e-01,
         -4.98058081e-01,  -4.32160288e-01,  -6.13680899e-01,
         -6.68697059e-01,  -2.78032994e+00,  -1.49310851e+00,
         -2.09862471e+00,  -2.08912468e+00,  -2.20983720e+00,
         -2.80304289e+00,  -2.82961416e+00,  -1.75434041e+00,
         -2.17259455e+00,  -2.21954417e+00,  -2.23375773e+00,
         -2.78085399e+00,  -3.10753560e+00,   7.32502460e-01,
          -0.05000000e+00,   1.82999998e-01,  -8.40334892e-02,
         -1.93231061e-01,  -3.23758841e-01,  -5.54996133e-01,
         -5.17783701e-01,  -8.00981641e-01,  -8.89853314e-02,
         -1.93129420e-01,  -3.30307811e-01,  -5.59639215e-01,
         -5.16421080e-01,  -8.07154715e-01,   5.63151896e-01,
         -4.85555589e-01,   4.61005628e-01,  -6.84546232e-01,
         -6.77374005e-01,  -1.13603234e+00,  -1.70841634e+00,
         -1.43679416e+00,   4.25571054e-01,  -6.52091861e-01,
         -7.51160443e-01,  -1.04742050e+00,  -1.79533529e+00,
         -1.32800865e+00],
       [  7.80558884e-01,   1.62972108e-01,   2.56182641e-01,
          8.16519186e-02,  -5.13627613e-03,   9.19231586e-03,
          1.71614304e-01,  -6.04884736e-02,   3.96177292e-01,
          1.61636069e-01,   6.72201742e-04,   9.82033089e-02,
          1.43492043e-01,   3.93710323e-02,   3.63897651e-01,
          2.60066915e+00,   1.17542839e+00,   2.77395582e+00,
          1.21126842e+00,   3.25060582e+00,   2.34962678e+00,
          4.04514980e+00,   3.74026036e+00,   2.98847461e+00,
          1.29100287e+00,   3.51810455e+00,   2.14481473e+00,
          4.40766239e+00,   3.49627423e+00,   7.92580068e-01,
          0.05000000e+00,   1.97999998e-01,   1.60339296e-01,
         -1.22687951e-01,   3.44305128e-01,  -3.84065241e-01,
          4.66087103e-01,  -5.45927703e-01,   1.53542519e-01,
         -1.25930950e-01,   3.30177665e-01,  -3.89418721e-01,
          4.49156046e-01,  -5.52706480e-01,   2.51450491e+00,
          4.87377495e-01,   2.66111302e+00,   6.63274586e-01,
          3.87225413e+00,   1.04573119e+00,   4.94935608e+00,
          1.29376113e+00,   2.62937951e+00,   6.23585522e-01,
          3.70267749e+00,   9.98043776e-01,   4.64572239e+00,
          1.26523149e+00]],
    "comment__": "Action scaling values to be used to scale values for the network",				     
"action_bounds": [[-2.57, -3.14, -1.57, -2.57, -3.14, -1.57],
				  [ 2.57,  0.5,   1.57,  2.57,  0.5,   1.57]],
	"comment__": "Set of discrete actions that can be sampled from",	
	"comment__": "   grab_elbow_desired_position, grab_arm_release_angle, next_grab_time, free_elbow_angle",								  
"discrete_actions": [[-0.1, -2.3, 0.15,  -1.35, -3.3, 1.2],
		             [-2.2, -0.74, 1.25, -0.11, -1.3, 0.7],
		             [-0.1, -0.4, 0.35,  -2.44, 0.3, -1.1],
		             [3.1, -2.45, 1.45,  -0.53, 1.3, 0.7],
		             [-0.1, -0.55, 0.35,  2.23, 0.3, 1.17],
		             [ 0.14, -0.45, -1.45, -0.15, 1.3, 1.27],
		             [ 1.1,  -3.5,  0.55, -0.12, 0.3, -0.7],
		             [-1.1,  -0.65, -2.35,  -0.67, 2.3, 0.7],
		             [-0.1,   3.5,  0.30, 3.71, 0.3, 1.17]], 
    "comment__": "Is action space continuous or discrete?",
"action_space_continuous":true,
    "comment__": "Should the method train on the validation set only",
"train_on_validation_set":true,
    "comment__": "Name of the type of simulator to use",
"environment_type": "terrainRLImitateBiped2D",
    "comment__": "Model type to use for the forward dynamics model",
"forward_dynamics_predictor": "simulator",
    "comment__": "Method to be used for the forward dynamics model is the model types uses a simulator",
"sampling_method": "SequentialMC",
    "comment__": "Use the action suggested by the policy to start the sampling method.",
"use_actor_policy_action_suggestion": true,
    "comment__": "If selecting from a uniform distribution the number of regularly distant samples to take / action dimension",
"num_uniform_action_samples": 2,
    "comment__": "Number of steps ahead the actions should be sampled",
"look_ahead_planning_steps": 2,
    "comment__": "How often to update the training data and plots wrt # of rounds",
"plotting_update_freq_num_rounds": 10,
    "comment__": "How often to save the training data and plotting data",
"saving_update_freq_num_rounds": 10,
    "comment__": "Number of treads that can be run in parallel during training",
"num_available_threads": 10,
    "comment__": "Length of the queues used to pass simulation data between the simulation workers and the learning agent(s).",
"queue_size_limit": 1000,
    "comment__": "Number of actions performed between training updates",
"sim_action_per_training_update": 8,
    "comment__": "Number of rounds of adaptive sampling",
"adaptive_samples": 25,
    "comment__": "Number of elite adaptive samples to keep between adaptive sampling rounds",
"num_adaptive_samples_to_keep": 50,
    "comment__": "Use the variance calculated from the policy network (calculated using dropout)",
"use_actor_policy_action_variance_suggestion": false,
    "comment__": "Method used for action exploration",
"exploration_method": "gaussian_random",
    "comment__": "Amount of dropout to use in the networks (if using a dropout network)",
"dropout_p": 0.1,
    "comment__": "Regularization weight for the policy network",
"regularization_weight": 0.00001,
    "comment__": "Some parameter for rmsprop stochastic gradient optimization method.",
"rho": 0.95,
    "comment__": "Some parameter for rmsprop stochastic gradient optimization method.",
"rms_epsilon": 0.001,
    "comment__": "Number of training updates before the target network is updated",
"steps_until_target_network_update": 500,
    "comment__": "Initial factor epsilon in multiplied by (This value will slowly be reduced during training)",
"epsilon_annealing": 0.8,
    "comment__": "Different ways of calculating the scaling method used normalize the input and outputs of the network from the bootstrapping samples. minmax, input and output are -mean/max-min. variance, input and output are -mean/(std*2), given, use the bounds provided in this file",
"state_normalization": "given",
"load_saved_model": false,
"critic_updates_per_actor_update": 1,
    "comment__": "weather or not to clamp actions to stay inside the action boundaries",
"clamp_actions_to_stay_inside_bounds": true,
    "comment__": "Number of initial actions to sample before calculating input/output scaling and starting to train.",
"bootsrap_samples": 4096,
    "comment__": "What method to use to select actions during bootstrapping",
"bootsrap_with_discrete_policy": true,
    "comment__": "That max number of action that can be take before the end of an episode/epoch",
"max_epoch_length": 500,
    "comment__": "If reward is below this bound it will not be put in the Experience Buffer",
"reward_lower_bound": -5.0,
    "comment__": "The number of training updates to perform for every action that is simulated",
"training_updates_per_sim_action": 1,
    "comment__": "Use the forward dynamics model to perform action exploration wrt to V -> fd > delta Action gradients",
"use_model_based_action_optimization": false,
    "comment__": "Flag for policy evaluation to swap in the task network from one model and the character/robot network from another",
"use_transfer_task_network": false,
    "comment__": "Add a large cost to actions that are suggested to be outside the action bounds.",
"penalize_actions_outside_bounds": false,
    "comment__": "Network type to use for the forward dynamics model",
"forward_dynamics_model_type": "Deep_NN",
    "comment__": "Whether or not to save the Experience memory after bootstrapping",
"save_experience_memory": false,
    "comment__": "Whether or not to train the policy and critic?",
"train_rl_learning": true,
    "comment__": "Force the character to start each new action in a good state, close to a good solution",
"use_back_on_track_forcing": false,
    "comment__": "draw/render the next state suggested by the forward dynamics model",
"visualize_forward_dynamics": false,
    "comment__": "Learning rate for the forward dynamics model",
"fd_learning_rate": 0.01,
    "comment__": "Whether or not to train the policy. Used for debugging",
"train_actor": true,
    "comment__": "Plot the terms for the critic as well (regularization and td error)",
"debug_critic": true,
    "comment__": "critic regularization weight",
"critic_regularization_weight": 0.000001,
    "comment__": "Critic learning rate",
"critic_learning_rate": 0.01,
    "comment__": "During evaluation plot of value function",
"visualize_expected_value": true,
    "comment__": "exponential decay value for use in reward function",
"target_velocity_decay":-0.75,
    "comment__": "Target velocity for controller",
"target_velocity":1.5,
    "comment__": "Number of terrain features for which convolutinoal filters should be used",
"num_terrain_features": 0,
    "comment__": "Initial tempurature for annealing of e-greedy exploration",
"initial_temperature": 1.25,
    "comment__": "epsilon lower limit",
"min_epsilon": 0.15,
    "comment__": "Whether or not to draw/render the simulation",
"shouldRender": false,
    "comment__": "Learning rate use for model based action exploration",
"action_learning_rate": 1.0,
    "comment__": "During model-based action exploration, Probability of random policy action",
"model_based_action_omega": 0.5,
	"comment__": "visualize the loss and regularization cost of the actor. Warning: might use a lot of memory",
"debug_actor": true,
	"comment__": "float type to use, if for example you want to train on the GPU use float32",
"float_type": "float32",
	"comment__": "What processor type to perform the training on [cpu|gpu]",
"training_processor_type": "cpu",
	"comment__": "optimizer is the type of optimization algorithm to use",
"optimizer": "adam",
	"comment__": "This setting uses a sampling method overtop of simulation",
"use_simulation_sampling": false,
	"comment__": "Variance scaling used for sampling",
"variance_scalling": 0.05,
	"comment__": "Whether or not to parameterize the control via the reward function, also adds parameters to input state",
"use_parameterized_control": false,
	"comment__": "parameter bounds for parameterized controller",
"controller_parameter_settings": {
		"velocity_bounds": [[0.5],[2.5]]
		},
	"comment__": "The parameter used to control the average change in the control parameters",
"average_parameter_change": 0.25,
	"comment__": "Whether or not to train the value function some output from the forward dynamics",
"train_critic_on_fd_output": false,
	"comment__": "Use to add an additional regularization term to prevent the network from moving to far from its previous values",
"use_previous_value_regularization": false,
	"comment__": "Controls the level of information that is printed to the terminal",
"print_level": "train",
	"comment__": "The type of function to apply over the controller target values [gaussian|abs]",
"reward_smoother": "gaussian",
	"comment__": "Weights for different components of the reward function",
"controller_reward_weights": {
		"velocity": 0.8,
		"torque": 0.05,
		"root_height": 0.05,
		"root_pitch": 0.1,
		"right_hand_x_pos": 0.0
		},
	"comment__": "Regularization weight for different between policy parameters and old policy parameters",
"previous_value_regularization_weight":  0.001,
	"comment__": "Random seed value for the simulation to use",
"random_seed": 1234,
	"comment__": "Makes a few changes to the flow of control in order for things to be on policy",
"on_policy": false,
	"comment__": "Whether or not to use a stochastic policy, This adds more outputs to the network and changes the way actions are sampled",
"use_stocastic_policy": false,
	"comment__": "Whether or  not to train the critic at all. Usually used for debugging",
"train_critic": true
}
