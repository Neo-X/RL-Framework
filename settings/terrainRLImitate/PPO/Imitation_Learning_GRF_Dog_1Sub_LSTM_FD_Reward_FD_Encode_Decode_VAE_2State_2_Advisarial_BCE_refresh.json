{
	"comment__": "Type of model/network to use for the actor and critic",
"model_type": "model.DeepNNKerasAdaptive.DeepNNKerasAdaptive",
    "comment__": "Learning algorithm to use",
"agent_name": "algorithm.PPO_KERAS.PPO_KERAS",
    "comment__": "Folder to store the training data in.",
"data_folder": "PD_Dog_2D_GRF_Viz3D_48x48_1Sub_Imitate_30FPS_DualState_v1/LSTM_FD_Dual_Reward_EncodeDecode_VAE_2State_FallReward2_T128_BCE_anneal_refresh_15",
	"comment": "initial probability of selecting a random action",
"epsilon": 1.0, 
	"comment": "initial probability of selecting a discrete random action",
"omega": 0.00,
    "comment__": "Batch size used for learning",
"batch_size": 256,
    "comment__": "Learning rate for the actor/policy",
"learning_rate": 0.0001,
    "comment__": "Config file for the simulator",
"sim_config_file": "PD_Dog_2D_GRF_Viz3D_48x48_1Sub_Imitate_30FPS_DualState_v1",
    "comment__": "Exploration distance use when randomly generating new actions",
"exploration_rate": 0.10,
    "comment__": "Number of rounds to perform before termination",
"rounds": 250,
    "comment__": "Number of epochs to perform per round",
"epochs": 4,
    "comment__": "Number of epoch/episode to evaluate the policy over",
"eval_epochs": 16,
    "comment__": "Discount factor used during learning",
"discount_factor": 0.95,
    "comment__": "Should the training be plotted during learning",
"visualize_learning": true,
    "comment__": "Whether or not to save the plotted data while learning",
"save_trainData": true,
    "comment__": "Whether or not to train a forward dynamics model as well",
"train_forward_dynamics": true,
    "comment__": "Bounds used for scaling rewards for networks",
"reward_bounds": [[0.0],[1.0]],
    "comment__": "Max length of the Experience memory",
"experience_length": 50000,
    "comment__": "Possible state bounds to be used for scaling states for networks",
"state_bounds": [[ 7.95523822e-02,  0.00000000e+00,  0.00000000e+00,
        -7.83543289e-02, -1.10616475e-01, -1.56643063e-01,
        -1.86334461e-01, -2.35587120e-01, -2.68086076e-01,
        -3.34493428e-01, -3.60005468e-01, -5.03996134e-01,
        -5.15546888e-01, -6.04501724e-01, -6.08953834e-01,
        -6.78187847e-01, -6.67220533e-01, -7.76468277e-01,
        -7.39124708e-01, -1.37684107e-01, -1.37726009e-01,
        -2.26138115e-01, -2.23707423e-01, -3.12168479e-01,
        -2.99655266e-01, -4.01388526e-01, -3.73514918e-01,
        -5.84803104e-01, -5.91589138e-01, -7.72780895e-01,
        -7.60081440e-01, -9.17064667e-01, -8.97167996e-01,
        -9.67009783e-01, -9.66494501e-01, -1.48952961e-01,
        -1.48880050e-01, -3.79502535e-01, -3.74145091e-01,
        -5.52240461e-01, -5.35660148e-01, -6.34923458e-01,
        -6.24771863e-01, -1.10412369e+01, -9.74592686e+00,
        -9.52480030e+00, -7.35041857e+00, -1.02671156e+01,
        -7.23704338e+00, -1.02912111e+01, -8.47438335e+00,
        -8.46304798e+00, -8.52091694e+00, -7.03367329e+00,
        -6.10271215e+00, -9.01600647e+00, -9.79513073e+00,
        -1.04422588e+01, -1.49727182e+01, -1.99353828e+01,
        -2.15220108e+01, -1.68096313e+01, -1.24465923e+01,
        -2.19763603e+01, -1.77917175e+01, -2.97956028e+01,
        -2.47129669e+01, -3.25852661e+01, -2.78363514e+01,
        -9.02378845e+00, -8.37555599e+00, -1.54825191e+01,
        -1.24555645e+01, -1.69695816e+01, -1.70434170e+01,
        -1.87106571e+01, -2.07357979e+01, -1.09962759e+01,
        -1.12429504e+01, -1.32853689e+01, -1.13613682e+01,
        -1.91185398e+01, -1.70513992e+01, -2.12004929e+01,
        -1.94705162e+01,  0.00000000,
		         0.00000000,  0.00000000,  0.00000000],
  [ 1.37172556e+00,  0.10000000e+00,  0.10000000e+00,
         1.10422611e-01,  1.09513342e-01,  1.84848011e-01,
         1.84736609e-01,  2.70185471e-01,  2.60633640e-01,
         3.59038770e-01,  3.60023409e-01,  5.31287909e-01,
         5.16184360e-01,  6.39957845e-01,  6.26346976e-01,
         7.09679842e-01,  7.07305044e-01,  8.16714168e-01,
         7.96454638e-01,  5.86266518e-02,  1.37728274e-01,
         1.44518852e-01,  2.25980222e-01,  2.33016014e-01,
         3.15530598e-01,  2.96110630e-01,  3.84242535e-01,
         6.44152686e-01,  5.97100936e-01,  8.63460049e-01,
         8.00202154e-01,  9.80518326e-01,  9.53046523e-01,
         1.05290312e+00,  1.03351387e+00,  1.31230116e-01,
         1.48714006e-01,  3.60064268e-01,  3.26700151e-01,
         5.11757851e-01,  4.85467076e-01,  5.98919153e-01,
         5.73025912e-01,  1.23416195e+01,  1.01247826e+01,
         9.20696831e+00,  8.07257175e+00,  9.45595932e+00,
         7.19830227e+00,  9.85241508e+00,  8.58963585e+00,
         9.88368225e+00,  7.53442717e+00,  8.29774952e+00,
         6.28802919e+00,  9.74507904e+00,  8.02820301e+00,
         1.19913683e+01,  1.10065050e+01,  1.78534126e+01,
         1.81091862e+01,  1.71969738e+01,  1.31736050e+01,
         2.24470406e+01,  1.50399370e+01,  2.86172619e+01,
         1.97994385e+01,  3.13581867e+01,  2.83356991e+01,
         1.10126877e+01,  1.13641243e+01,  1.65216789e+01,
         1.21766357e+01,  1.73357315e+01,  1.38179388e+01,
         2.08329220e+01,  1.66723232e+01,  1.06338043e+01,
         9.89536667e+00,  1.34826746e+01,  1.16450844e+01,
         1.57271585e+01,  1.78111782e+01,  1.92559204e+01,
         2.18602295e+01,  1.00000000,
		         1.00000000,  1.00000000,  1.00000000]],
        "comment__": "Action scaling values to be used to scale values for the network",				     
"action_bounds": [[ -6.28      ,  -6.28      ,  -6.28      ,  -6.28      ,
        -6.28      ,  -6.28      ,  -6.28      ,  -6.28      ,
       -12.56637061, -12.56637061, -12.56637061, -12.56637061,
        -6.28      ,  -4.71      ,  -4.71      ,  -6.28      ,
        -6.28      ,  -7.85      ,  -4.71      ,  -6.28      ],
				  [ 6.28      ,  6.28      ,  6.28      ,  6.28      ,  6.28      ,
        6.28      ,  6.28      ,  6.28      , 12.56637061, 12.56637061,
       12.56637061, 12.56637061,  6.28      ,  7.85      ,  7.85      ,
        6.28      ,  6.28      ,  4.71      ,  7.85      ,  6.28      ]],
    "comment__": "Set of discrete actions that can be sampled from",				     
"discrete_actions": [[-0.92, 2.8],
                    [0.02, 3.0],
                    [0.3, 4.5],
                    [-0.4, 4.2],
                    [0.8, 4.9],
                    [-0.7, 3.0], 
                    [0.7, 4.6],
                    [-0.5, 2.6],
                    [-0.2, 3.6]],
    "comment__": "Is action space continuous or discrete?",
"action_space_continuous":true,
    "comment__": "Should the method train on the validation set only",
"train_on_validation_set":false,
    "comment__": "Name of the type of simulator to use",
"environment_type": "terrainRLSim",
    "comment__": "Model type to use for the forward dynamics model",
"forward_dynamics_predictor": "network",
    "comment__": "Method to be used for the forward dynamics model is the model types uses a simulator",
"sampling_method": "SequentialMC",
    "comment__": "Use the action suggested by the policy to start the sampling method.",
"use_actor_policy_action_suggestion": true,
    "comment__": "If selecting from a uniform distribution the number of regularly distant samples to take / action dimension",
"num_uniform_action_samples": 3,
    "comment__": "Number of steps ahead the actions should be sampled",
"look_ahead_planning_steps": 2,
    "comment__": "How often to update the training data and plots wrt # of rounds",
"plotting_update_freq_num_rounds": 5,
    "comment__": "How often to save the training data and plotting data",
"saving_update_freq_num_rounds": 5,
    "comment__": "Number of treads that can be run in parallel during training",
"num_available_threads": 4,
    "comment__": "Length of the queues used to pass simulation data between the simulation workers and the learning agent(s).",
"queue_size_limit": 10,
    "comment__": "Number of actions performed between training updates",
"sim_action_per_training_update": 8,
    "comment__": "Number of rounds of adaptive sampling",
"adaptive_samples": 5,
    "comment__": "Number of elite adaptive samples to keep between adaptive sampling rounds",
"num_adaptive_samples_to_keep": 50,
    "comment__": "Use the variance calculated from the policy network (calculated using dropout)",
"use_actor_policy_action_variance_suggestion": false,
    "comment__": "Method used for action exploration [gaussian_random|OrnsteinUhlenbeck]",
"exploration_method": "gaussian_network",
    "comment__": "Amount of dropout to use in the networks (if using a dropout network)",
"dropout_p": 0.0,
    "comment__": "Regularization weight for the policy network",
"regularization_weight": 0.000001,
    "comment__": "Some parameter for rmsprop stocastic gradient optimization method.",
"rho": 0.95,
    "comment__": "Some paraameter for rmsprop stocastic gradient optimization method.",
"rms_epsilon": 0.000001,
    "comment__": "Number of training updates before the target network is updated",
"steps_until_target_network_update": 1000000000,
    "comment__": "Initial factor epsilon in multiplied by (This value will slowly be reduced during training)",
"epsilon_annealing": 0.8,
    "comment__": "Different ways of calculating the scaling method used normalize the input and outputs of the network from the bootstrapping samples. minmax, input and output are -mean/max-min. variance, input and output are -mean/(std*2), given, use the bounds provided in this file",
"state_normalization": "adaptive",
    "comment__": "load a pretrained model for the controller",
"load_saved_model": false,
	"comment__": "Number of updates the critic should perform per actor update",
"critic_updates_per_actor_update": 4,
    "comment__": "weather or not to clamp actions to stay inside the action boundaries",
"clamp_actions_to_stay_inside_bounds": false,
    "comment__": "Number of initial actions to sample before calculating input/output scaling and starting to train.",
"bootstrap_samples": 10000,
    "comment__": "What method to use to select actions during bootstrapping",
"bootsrap_with_discrete_policy": true,
    "comment__": "That max number of action that can be take before the end of an episode/epoch",
"max_epoch_length": 64,
    "comment__": "If reward is below this bound it will not be put in the Experience Buffer",
"reward_lower_bound": -300.5,
    "comment__": "Enable guided policy search. Uses MCMC sampling ahead in time to select the best action to keep",
"use_guided_policy_search" : false,
    "comment__": "The number of training updates to perform for every action that is simulated",
"training_updates_per_sim_action": 1,
    "comment__": "Use The forward dynamics simulator as a way of sampling suggested actions for exploration",
"use_sampling_exploration": false,
    "comment__": "Use the forward dyanmics model to perform action exploration wrt to V -> fd > delta Action gradients",
"use_model_based_action_optimization": false,
    "comment__": "Flag for policy evaluation to swap in the task network from one model and the character/robot network from another",
"use_transfer_task_network": false,
    "comment__": "Add a large cost to actions that are suggested to be outside the action bounds.",
"penalize_actions_outside_bounds": false,
    "comment__": "Network type to use for the forward dynamics model",
"forward_dynamics_model_type": "model.FDNNKerasAdaptive.FDNNKerasAdaptive",
    "comment__": "Whether or not to save the Experience memory after bootstrapping",
"save_experience_memory": false,
    "comment__": "Whether or not to train the policy and critic?",
"train_rl_learning": true,
    "comment__": "Force the character to start each new action in a good state, close to a good solution",
"use_back_on_track_forcing": false,
    "comment__": "draw/render the next state suggested by the forward dynamics model",
"visualize_forward_dynamics": false,
    "comment__": "Learning rate for the forward dynamics model",
"fd_learning_rate": 0.0005,
    "comment__": "Whether or not to train the policy. Used for debugging",
"train_actor": true,
    "comment__": "Plot the terms for the critic as well (regularization and td error)",
"debug_critic": false,
    "comment__": "critic regularization weight",
"critic_regularization_weight": 0.00001,
    "comment__": "Critic learning rate",
"critic_learning_rate": 0.001,
    "comment__": "During evaluation plot of value function",
"visualize_expected_value": true,
    "comment__": "exponential decay value for use in reward function",
"target_velocity_decay":-2.0,
    "comment__": "Target velocity for controller",
"target_velocity":0.0,
    "comment__": "Number of terrain features for which convolutinoal filters should be used",
"num_terrain_features": 0,
    "comment__": "Initial tempurature for annealing of e-greedy exploration",
"initial_temperature": 1.0,
    "comment__": "epsilon lower limit",
"min_epsilon": 0.05,
    "comment__": "Whether or not to draw/render the simulation",
"shouldRender": true,
    "comment__": "Learning rate use for model based action exploration",
"action_learning_rate": 0.50,
    "comment__": "During model-based action exploration, Probability of a random action being generated from MBAE,",
"model_based_action_omega": 0.5,
	"comment__": "visualize the loss and regularization cost of the actor. Warning: might use a lot of memory",
"debug_actor": true,
	"comment__": "float type to use, if for example you want to train on the GPU use float32",
"float_type": "float32",
	"comment__": "What processor type to perform the training on [cpu|gpu]",
"training_processor_type": "cpu",
	"comment__": "optimizer is the type of optimization algorithm to use",
"optimizer": "adam",
	"comment__": "This setting uses a sampling method overtop of simulation",
"use_simulation_sampling": false,
	"comment__": "Variance scaling used for sampling",
"variance_scalling": 0.1,
	"comment__": "Whether or not to parameterize the control via the reward function, also adds parameters to input state",
"use_parameterized_control": false,
	"comment__": "parameter bounds for parameterized controller",
"controller_parameter_settings": {
		"velocity_bounds": [[0.5],[2.5]]
		},
	"comment__": "The parameter used to control the average change in the control parameters",
"average_parameter_change": 0.25,
	"comment__": "Whether or not to train the value function some output from the forward dynamics",
"train_critic_on_fd_output": true,
	"comment__": "Use to add an additional regularization term to prevent the network from moving to far from its previous values",
"use_previous_value_regularization": false,
	"comment__": "Controls the level of information that is printed to the terminal",
"print_level": "train",
	"comment__": "print level descriptions",
"print_levels": {
		"debug": 1,
		"train": 0,
		"hyper_train": -1,
		"testing_sim": -2
		},
	"comment__": "The type of function to apply over the controller target values [gaussian|abs]",
"reward_smoother": "gaussian",
	"comment__": "Weights for different components of the reward function",
"controller_reward_weights": {
		"velocity": 0.8,
		"torque": 0.05,
		"root_height": 0.05,
		"root_pitch": 0.1,
		"right_hand_x_pos": 0.0
		},
	"comment__": "Regularization weight for different between policy parameters and old policy parameters",
"previous_value_regularization_weight":  0.01,
	"comment__": "Random seed value for the simulation to use",
"random_seed": 1234,
	"comment__": "KL divergence threshold between policy updates",
"kl_divergence_threshold": 0.1,
	"comment__": "Makes a few changes to the flow of control in order for things to be on policy [true|fast]",
"on_policy": "fast",
	"comment__": "Whether or not to use a stochastic policy, This adds more outputs to the network and changes the way actions are sampled",
"use_stochastic_policy": false,
	"comment__": "Whether or  not to train the critic at all. Usually used for debugging",
"train_critic": true,
	"comment__": "What type of regularization to use",
"regularization_type": "kl",
	"comment__": "Whether or not to collects tuples in batches, this can be good for multi-threading or computing furture discounted reward",
"collect_tuples_in_batches":false,
	"comment__": "Whether or not the controller should be reset to a new epoch when a fall (fallen into some kind of non-recoverable state) has occured",
"reset_on_fall": true,
	"comment__": "Whether a model of the reward r <- R(s,a) should be trained",
"train_reward_predictor": true,
	"comment__": "How many gradient steps model based action exploration should take",
"num_mbae_steps": 1,
    "comment__": "Whether or not the actor buffer should be fixed to process batches of a certain size",
"fix_actor_batch_size": true,
	"comment__": "The number of critic updates that are done between every dyna update step.",
"dyna_update_lag_steps": 5,
	"comment__": "Use generalized advantage estimation",
"use_GAE": true,
	"comment__": "generalized advantage estimation lambda in [0,1], when =0, this is just a one step return (lots of bias), when =1, use only data no variance reduction",
"GAE_lambda": 0.95,
	"comment__": "Whether or not to take a couple of on policy updates",
"use_multiple_policy_updates": false,	
	"comment__": "Whether or not to clear the experience memory after each update is done, for pure on policy methods",
"clear_exp_mem_on_poli": true,
	"comment__": "Disable scaling for inputs and outputs of networks",
"disable_parameter_scaling": false,
	"comment__": "Train a state encoding as well.",
"train_state_encoding": false,
	"comment__": "std entropy weight to help encourage exploration",
"std_entropy_weight": 0.01,
	"comment__": "policy loss function weight, to help balance policy loss vs value function loss",
"policy_loss_weight": 1.0,
	"comment__": "number of on policy rollouts to perform per epoch",
"num_on_policy_rollouts": 32,
	"comment__": "Don't weight policy updates wrt the advantage of the action",
"dont_use_advantage": false,
	"comment__": "PPO should use seperate networks for the value and policy functions",
"use_random_actions_for_MBAE": false,
	"comment__": "Only use exploratory actions to update the policy",
"only_use_exp_actions_for_poli_updates": true,
	"comment__": "Multiply the MBAE action by a sample from a uniform distribution, to allow the action to vary in magnitude",
"randomize_MBAE_action_length": true,
	"comment__": "Load a saved and pre-trained forward dynamics model",
"load_saved_fd_model": false,
	"comment__": "Whether or not to use a stochastic forward dynamics model",
"use_stochastic_forward_dynamics": false,
	"comment__": "Normalize the advantage used for gradient calculations wrt the discount factor and scale of reward",
"normalize_advantage": true,
	"comment__": "use a special annealing schedule that will being at 1 and end at 0",
"annealing_schedule": "linear",
	"comment__": "Use the annealing schedule in on policy learning",
"anneal_on_policy": true,
	"comment__": "Use json network description",
"network_description_type": "json",
	"comment__": "A list describing the size of the layers for the NN",
"critic_network_layer_sizes": [ 
								{"layer_type": "Dropout", "rate": 0.2},
								{"layer_type": "Dense", "units": 256, "use_bias": true},
								{"layer_type": "activation", "activation_type": "leaky_rectify" },
								{"layer_type": "Dropout", "rate": 0.2},
								{"layer_type": "Dense", "units": 128, "use_bias": true},
								{"layer_type": "activation", "activation_type": "leaky_rectify" },
								{"layer_type": "Dropout", "rate": 0.2}
								],
	"comment__": "A list describing the size of the layers for the NN",
"policy_network_layer_sizes": [ 
								{"layer_type": "Dense", "units": 256, "use_bias": true},
								{"layer_type": "activation", "activation_type": "leaky_rectify" },
								{"layer_type": "Dense", "units": 128, "use_bias": true},
								{"layer_type": "activation", "activation_type": "leaky_rectify" }
								],
	"comment__": "Activation function to use for the std part of the policy",
"_last_std_policy_layer_activation_type": "sigmoid",
	"comment__": "type of activation to use",
"activation_type": "leaky_rectify",
	"comment__": "type of activation to use for policy",
"policy_activation_type": "leaky_rectify",
	"comment__": "last policy type of activation to use",
"last_policy_layer_activation_type": "tanh",
	"comment__": "Use a different off-policy experience memory for the fd learning",
"keep_seperate_fd_exp_buffer": true,
	"comment__": "Train a GAN to do something....",
"train_gan": false,
	"comment__": "Train GAN with gaussian noise",
"train_gan_with_gaussian_noise": true,
	"comment__": "This overrides all of the envs to be a single one from the list of sim environments",
"override_sim_env_id": false,
	"comment__": "Prevents the critic from including MBAE actions in it's policy estimation.",
"give_mbae_actions_to_critic": false,
	"comment__": "The interpolation parameter used adjust the target network for DDPG",
"target_net_interp_weight": 0.001,
	"comment__": "Batch size for the value function network",
"value_function_batch_size": 64,
	"comment__": "Train the critic off of the data stored in the exp for the fd model",
"train_critic_with_fd_data": true,
	"comment__": "Additional on-policy training updates",
"additional_on_policy_training_updates": 4,
	"comment__": "Use average of policy std as mbae learning rate",
"use_std_avg_as_mbae_learning_rate": true,
	"comment__": "Use the on-policy value function to compute state gradients instead of off-policy one for SMBAE.",
"use_extra_value_for_MBAE_state_grads": true,
	"comment__": "Performs MBAE random sampling at the level of episodes instead of transitions",
"perform_mbae_episode_sampling": false,
	"comment__": "Using a single network for all functions",
"use_single_network": false,
	"comment__": "Anneal the std of the policy as well",
"anneal_policy_std": false,
	"comment__": "anneal the probability of performing explorative actions, set to >= 1.0 to perform only exploration actions",
"anneal_exploration": 1.0,
	"comment__": "If true the simulation will terminate after an early termination or when max time steps is reached, otherwise max_time_steps sample are collected",
"sample_single_trajectories": true,
	"comment__": "Network settings",
"network_settings": {
		"comment__": "Whether or not the network should include pooling layers between convolutions.",
	"perform_convolution_pooling": false,
		"comment__": "Change where the network splits for a single network model",
	"split_single_net_earlier": false,
		"comment__": "Use CoordConv convolutional layers.",
	"use_coordconv_layers": true
},
	"comment__": "Run some initial training steps to pretrain the critic before starting policy training",
"pretrain_critic": 0,
	"comment__": "Policy distribution change factor, if beyond this updates are not performed",
"ppo_et_factor": 1.1,
	"comment__": "higher level controller timestep",
"hlc_timestep": 1,
	"comment__": "Which backend to use for keras [theano|tensorflow]",
"learning_backend": "tensorflow",
	"comment__": "Use an L2 loss over for the value function over the G_t from rollouts (no TD loss)",
"dont_use_td_learning": false,
	"comment__": "Anneal the learning rate",
"anneal_learning_rate": false,
	"comment__": "Data ordering format for convolutions",
"image_data_format": "channels_first",
	"comment__": "Size and shape of terrain",
"terrain_shape": [1,48,48],
    "comment__": "Number of terrain features for which convolutinoal filters should be used",
"num_terrain_features": 0,
	"comment__": "specify the learning algorithm to use for the fd model",
"fd_algorithm" : "algorithm.SiameseNetworkBCEMultiHeadDecodeVAE.SiameseNetworkBCEMultiHeadDecodeVAE",
	"comment__": "A list describing the size of the layers for the NN",
"fd_network_layer_sizes": [ 
							{"layer_type": "slice"},
							{"layer_type": "Reshape", "target_shape": [1,48,48]},
							{"layer_type": "GaussianNoise", "stddev": 0.05},
							{"layer_type": "coordconv2d"},
							{"layer_type": "conv2d", "filters": 8, "kernel_size": [6,6], "strides": [2,2], "use_bias": false, "padding": "same"},
							{"layer_type": "BatchNormalization"},
							{"layer_type": "activation", "activation_type": "leaky_rectify" },
							{"layer_type": "Dropout", "rate": 0.2},
							{"layer_type": "coordconv2d"},
							{"layer_type": "conv2d", "filters": 8, "kernel_size": [4,4], "strides": [2,2], "use_bias": false, "padding": "same"},
							{"layer_type": "BatchNormalization"},
							{"layer_type": "activation", "activation_type": "leaky_rectify" },
							{"layer_type": "Dropout", "rate": 0.2},
							{"layer_type": "Flatten"},
							{"layer_type": "Dense", "units": 128, "use_bias": true},
							{"layer_type": "Concatenate"},
							{"layer_type": "activation", "activation_type": "leaky_rectify" }
							],
	"comment__": "A list describing the size of the layers for the NN",
"reward_network_layer_sizes": [
							{"layer_type": "Input", "shape": ["none", "none", 64]},
							{"layer_type": "LayerNormalization"},
							{"layer_type": "GRU", "units": 128, "return_sequences": true, "return_state": true, "use_bias": true, "dropout": 0.2, "recurrent_dropout": 0.2}
							],
	"comment__": "type of activation to use",
"reward_activation_type": "leaky_rectify",
	"comment__": "type of activation to use for policy",
"fd_policy_activation_type": "leaky_rectify",
	"comment__": "last policy type of activation to use",
"last_fd_layer_activation_type": "sigmoid",
	"comment__": "The number of fd network to be done per actor update.",
"fd_updates_per_actor_update": 1.0,
	"comment__": "Lets the network models handle the training and batch processing",
"model_perform_batch_training": false,
	"comment__": "Leave off the final layer of the network that would result in a state sized output",
"fd_network_leave_off_end": true,
	"comment__": "Dropout for fd network",
"fd_network_dropout": 0.0,
	"comment__": "Use learned reward function",
"use_learned_reward_function": false,
	"comment__": "Mix different state description types, used for debugging visual imitation learning",
"use_dual_state_representations": true,
	"comment__": "Use vis state for policy instead of dense state",
"use_viz_for_policy": false,
	"comment__": "Dense state size",
"dense_state_size": 157,
	"comment__": "Size and shape of terrain",
"fd_terrain_shape": [1,48,48],
    "comment__": "Number of terrain features for which convolutinoal filters should be used",
"fd_num_terrain_features": 2304,
	"comment__": "replace the viz state in the next state with the state from the imitation agent",
"replace_next_state_with_imitation_viz_state": true,
	"comment__": "Type of smoothing funtion to filter the learned distance metric by",
"learned_reward_smoother": "bce",
	"comment__": "Just train the fd model in an LSTM fashion",
"train_LSTM_FD": false,
	"comment__": "Use a stateful LSTM",
"train_LSTM_FD_stateful": false,
	"comment__": "batch size for lstm fd models",
"lstm_batch_size": [4, 32],
	"comment__": "experience size for fd network",
"fd_experience_length": 500,
	"comment__": "A mapping from working_id to task id",
"worker_to_task_mapping": [0, 1, 2, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
	"comment__": "A small offset for comparing states that we now are imperfect comparisons",
"imperfect_compare_offset": 0.02,
	"comment__": "Force the simulation workers to use the CPU only for network models",
"force_sim_net_to_cpu": false,
	"comment__": "Just train the fd model in an LSTM fashion",
"train_LSTM_Reward": true,
	"comment__": "Leave off the final layer of the reward network that would result in a state sized output",
"reward_network_leave_off_end": true,
	"comment__": "Episode simulation time out",
"simulation_timeout": 1800,
	"comment__": "Include some randomly sized sequences when training lstms",
"use_random_sequence_length_for_lstm": true,
	"comment__": "Use a sparse sequence based reward",
"use_sparse_sequence_based_reward": false,
	"comment__": "Add label noise to Discriminator/Simease network training",
"add_label_noise": 0.05,
	"comment__": "Make earlier and shorter lstm training batches more probable.",
"shorter_smaller_rnn_batches": false,
	"comment__": "When the agent falls give it a bad reward 1/(1-discount_fator)",
"use_fall_reward_shaping": true,
	"comment__": "Run some initial training steps to pretrain the fd before starting policy training",
"pretrain_fd": 0,
	"comment__": "Add velocity state to image state",
"append_camera_velocity_state": "3D",
	"comment__": "The shortest sequence the RNN network will be trained on",
"min_sequece_length": 3,
	"comment__": "The type of distance function the fd learning model should use to compute differences.",
"fd_distance_function": "l1",
	"comment__": "image noise factor that is added to the frames captured from the simulations",
"image_noise_scale": 0.00,
	"comment__": "Return sequences from lstm training",
"return_rnn_sequence": false,
	"comment__": "condition siamese network on RNN internal state as well.",
"condition_on_rnn_internal_state": false,
	"comment__": "Train the fd network as well",
"train_lstm_fd_and_reward_together": false,
	"comment__": "Train the fd network as well",
"train_lstm_fd_and_reward_and_decoder_together": true,
	"comment__": "A list describing the size of the layers for the NN",
"decoder_network_layer_sizes": [ 
								{"layer_type": "slice", "slice_index": 61, "slice_label": "out_vel" },
								{"layer_type": "Dense", "units": 1152, "use_bias": false},
								{"layer_type": "BatchNormalization"},
								{"layer_type": "activation", "activation_type": "leaky_rectify" },
								{"layer_type": "Dropout", "rate": 0.2},
								{"layer_type": "Reshape", "target_shape": [8,12,12]},
								{"layer_type": "deconv2d", "filters": 8, "kernel_size": [4,4], "strides": [2,2], "use_bias": false, "padding": "same"},
								{"layer_type": "BatchNormalization"},
								{"layer_type": "activation", "activation_type": "leaky_rectify" },
								{"layer_type": "Dropout", "rate": 0.2},
								{"layer_type": "deconv2d", "filters": 1, "kernel_size": [6,6], "strides": [2,2], "use_bias": false, "padding": "same"},
								{"layer_type": "BatchNormalization"},
								{"layer_type": "activation", "activation_type": "leaky_rectify" },
								{"layer_type": "Dropout", "rate": 0.2},
								{"layer_type": "Reshape", "target_shape": [2304]},
								{"layer_type": "Concatenate", "slice_label": "out_vel"}
							],
	"comment__": "A list describing the size of the layers for the NN",
"decoder_network_layer_sizes2": [
							{"layer_type": "Input", "shape": ["none", "none", 64]},
							{"layer_type": "GRU", "units": 128, "return_sequences": true, "return_state": false, "use_bias": false, "dropout": 0.2, "recurrent_dropout": 0.2, "activation": "tanh"},
							{"layer_type": "TimeDistributedConv", "net_info":
							[ 
								{"layer_type": "Dense", "units": 64, "use_bias": true},
								{"layer_type": "activation", "activation_type": "linear" }
							]}
							],
	"comment__": "The encoding vector size used for the fd models",
"encoding_vector_size": 64,
	"comment__": "Inform the system that a encoder decoder model is being used",
"using_encoder_decoder_fd": false,
	"comment__": "Inform the system that a encoder decoder model is being used",
"use_decoder_fd": false,
	"comment__": "Use advisarial comparisons between agent and expert data",
"include_agent_imitator_pairs": true,
	"comment__": "Seperate the positive and negative pairs and perform two batches over the data.",
"seperate_posandneg_pairs": true,
	"comment__": "Add noise to the target noise values",
"target_noise_scale": 0.05,
"rnn_updates": 32,	
	"comment__": "Data for using comet ML for logging learning experiments",
"experiment_logging":
	{"use_comet": true,
	"project_name": "vizimitation"},
	"comment__": "Save a video from the evaluation runs",
"save_eval_video": true,
"anneal_dual_reward": true,
"log_model_gen_seq_output": true,
"refresh_rewards": true,
"virl_loss_weights": [0.5, 0.3, 0.075, 0.075, 0.025, 0.025],
"virl_loss_weights_": [0.0, 1.0, 0.00, 0.00, 0.00, 0.00],
"use_max_T_annealing": true,
"refresh_rewards_rl_method": "fd"
}