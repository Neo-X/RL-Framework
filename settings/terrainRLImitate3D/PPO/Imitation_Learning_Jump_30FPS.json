{
	"comment__": "Type of model/network to use for the actor and critic",
"model_type": "model.DeepNNKerasAdaptive.DeepNNKerasAdaptive",
    "comment__": "Learning algorithm to use",
"agent_name": "algorithm.PPO_KERAS.PPO_KERAS",
    "comment__": "Folder to store the training data in.",
"data_folder": "PD_Humanoid_GRF_3D_Jump_Viz3D_1Sub_Imitate_30FPS_v0_v0/T128_23",
	"comment": "initial probability of selecting a random action",
"epsilon": 1.0, 
	"comment": "initial probability of selecting a discrete random action",
"omega": 0.00,
    "comment__": "Batch size used for learning",
"batch_size": 256,
    "comment__": "Learning rate for the actor/policy",
"learning_rate": 0.0001,
    "comment__": "Config file for the simulator",
"sim_config_file": "PD_Humanoid_GRF_3D_Jump_Viz3D_1Sub_Imitate_30FPS_v0_v0",
    "comment__": "Exploration distance use when randomly generating new actions",
"exploration_rate": 0.25,
    "comment__": "Number of rounds to perform before termination",
"rounds": 500,
    "comment__": "Number of epochs to perform per round",
"epochs": 4,
    "comment__": "Number of epoch/episode to evaluate the policy over",
"eval_epochs": 32,
    "comment__": "Discount factor used during learning",
"discount_factor": 0.95,
    "comment__": "Should the training be plotted during learning",
"visualize_learning": true,
    "comment__": "Whether or not to save the plotted data while learning",
"save_trainData": true,
    "comment__": "Whether or not to train a forward dynamics model as well",
"train_forward_dynamics": false,
    "comment__": "Bounds used for scaling rewards for networks",
"reward_bounds": [[0.0],[1.0]],
    "comment__": "Max length of the Experience memory",
"expereince_length": 25000,
    "comment__": "Possible state bounds to be used for scaling states for networks",
"state_bounds": [[ 4.88499906e-01,  7.61015913e-04,  3.01301218e-02,
        -3.46773881e-02,  8.01017958e-01, -2.86482509e-01,
        -1.09923024e-01, -3.56663478e-01,  2.34766166e-03,
         1.55248654e-01, -1.75177374e-01,  7.93267378e-01,
        -2.84551368e-01, -1.37041442e-01, -3.57283902e-01,
         9.63095657e-05,  2.77920990e-01, -3.09461727e-01,
         7.91267702e-01, -2.85872632e-01, -1.48556828e-01,
        -3.62254557e-01, -1.21168055e-01, -2.23176115e-01,
        -6.56410600e-03,  8.02462544e-01, -2.95228993e-01,
        -1.35059482e-01, -3.53208676e-01, -3.70280979e-01,
        -6.34466627e-01, -1.56197289e-01,  7.79563962e-01,
        -2.99220298e-01, -1.27382934e-01, -4.29936091e-01,
        -4.91121891e-01, -8.70168117e-01, -2.38890995e-01,
         8.02046171e-01, -2.67912982e-01, -1.48126734e-01,
        -3.53790112e-01, -3.64814432e-02,  1.65435662e-01,
        -2.66347540e-02,  7.86734441e-01, -3.18139200e-01,
        -1.27999696e-01, -3.78405735e-01, -7.14644964e-02,
         1.37133196e-02,  8.02855838e-02,  7.98743469e-01,
        -3.10648792e-01, -1.32863841e-01, -3.42630019e-01,
        -1.16301431e-01, -1.00154159e-01,  1.01502763e-01,
         4.87593452e-01, -2.49677734e-01, -1.72390001e-01,
        -4.57541640e-01, -1.21970375e-01, -2.23214812e-01,
        -1.53406900e-01,  8.01088262e-01, -3.00939323e-01,
        -1.39586695e-01, -3.49790782e-01, -3.63354646e-01,
        -6.41603883e-01, -2.97626883e-01,  7.80672449e-01,
        -3.06243831e-01, -1.35709871e-01, -4.16172430e-01,
        -4.75773899e-01, -8.80439027e-01, -3.75361150e-01,
         8.11347430e-01, -2.43079819e-01, -1.59871481e-01,
        -3.29622320e-01, -4.51444267e-02,  1.26337962e-01,
        -3.30413882e-01,  7.89088656e-01, -2.65884730e-01,
        -1.64579766e-01, -3.66602988e-01, -7.85809744e-02,
        -1.92215289e-02, -2.56055641e-01,  8.00100302e-01,
        -2.60416286e-01, -1.75476174e-01, -3.25063008e-01,
        -1.18887133e-01, -1.23704362e-01, -2.60904085e-01,
         4.34592442e-01, -2.09060377e-01, -2.08461478e-01,
        -4.27870505e-01,  1.94410398e-01, -1.99998983e+00,
        -7.97310604e-01, -3.08346128e+00, -3.20366109e+00,
        -3.54383561e+00,  3.95868889e-01, -2.71014874e+00,
        -9.47650121e-01, -3.45750686e+00, -3.21407439e+00,
        -4.19129300e+00,  5.90277481e-02, -3.52432781e+00,
        -1.37577744e+00, -4.84818990e+00, -4.75497113e+00,
        -5.33592423e+00, -1.72391009e-01, -1.36362855e+00,
        -6.91491795e-01, -2.79839962e+00, -3.48670463e+00,
        -3.88461060e+00, -6.59786924e-01, -6.49311514e-01,
        -7.09979441e-01, -3.00278442e+00, -3.60945021e+00,
        -4.26439630e+00, -1.18977604e+00, -6.37254225e-01,
        -8.21608370e-01, -4.47347332e+00, -4.61328498e+00,
        -4.88110371e+00,  7.52931343e-02, -2.65616639e+00,
        -1.19687994e+00, -3.74979618e+00, -4.29527668e+00,
        -4.77607959e+00, -3.90615443e-01, -2.22159302e+00,
        -1.31387943e+00, -3.87538063e+00, -4.35851527e+00,
        -5.35708286e+00, -8.77701188e-01, -2.14040592e+00,
        -1.49792060e+00, -8.28630449e+00, -7.92350573e+00,
        -1.56951425e+01, -7.81196880e-02, -1.41791518e+00,
        -5.55702619e-01, -2.89333224e+00, -3.75761532e+00,
        -3.89693896e+00, -5.68212685e-01, -6.14291392e-01,
        -5.92563642e-01, -2.97287037e+00, -3.80366161e+00,
        -4.32632498e+00, -1.05100203e+00, -4.98476090e-01,
        -7.57454974e-01, -4.42985236e+00, -4.63946631e+00,
        -4.80529559e+00,  2.99792376e-01, -2.82183291e+00,
        -8.96156545e-01, -4.28410116e+00, -3.93531482e+00,
        -4.82054640e+00, -9.19973554e-02, -2.33319260e+00,
        -9.56303866e-01, -4.30684374e+00, -3.88956952e+00,
        -5.21953508e+00, -5.42323004e-01, -2.20338421e+00,
        -1.12804978e+00, -8.05580139e+00, -8.88961966e+00,
        -1.37933902e+01,
        -0.00000000e+00, -0.00000000e+00, -0.00000000e+00,
        -0.00000000e+00, -0.00000000e+00, -0.00000000e+00,
        -0.00000000e+00, -0.00000000e+00, -0.00000000e+00,
        -0.00000000e+00, -0.00000000e+00, -0.00000000e+00],
  [ 9.15790492e-01,  3.94313722e-02,  7.36535061e-02,
         2.66346282e-02,  1.03624237e+00,  2.20652981e-01,
         1.17475704e-01, -2.68249712e-02,  1.98352773e-01,
         3.73224032e-01,  1.34804169e-01,  1.03582744e+00,
         2.18879846e-01,  1.48752233e-01, -1.87390938e-02,
         3.52286704e-01,  6.62041170e-01,  2.40074766e-01,
         1.03110640e+00,  2.29057137e-01,  1.52116852e-01,
        -9.36362808e-03,  2.36295347e-02, -7.96121049e-02,
         1.80469186e-01,  1.02953901e+00,  2.12907654e-01,
         1.42366680e-01,  1.86486751e-02,  1.15060858e-02,
        -2.50446405e-01,  3.88313382e-01,  1.01440105e+00,
         2.19436032e-01,  1.30724385e-01, -8.61700903e-02,
         2.62670422e-02, -3.61070375e-01,  5.01849806e-01,
         1.03496935e+00,  1.96044549e-01,  1.67960237e-01,
         5.47388559e-03,  1.92388638e-01,  3.81711494e-01,
         3.10008280e-01,  1.02786933e+00,  1.88278966e-01,
         1.68005912e-01, -8.43967753e-03,  1.07276436e-01,
         1.59860889e-01,  2.61924619e-01,  1.02904692e+00,
         1.84319451e-01,  1.82048865e-01,  6.63706231e-02,
         1.01426411e-01,  6.59391610e-02,  2.71740086e-01,
         1.02864607e+00,  1.81213316e-01,  2.37434164e-01,
         5.59933067e-01,  2.62564468e-02, -9.86289323e-02,
         4.42248582e-02,  1.03119620e+00,  2.03043610e-01,
         1.42200080e-01,  3.22786408e-02,  3.20985564e-02,
        -2.70722646e-01,  2.55065920e-01,  1.02162108e+00,
         2.06095417e-01,  1.29690598e-01, -4.63032972e-02,
         6.30211344e-02, -3.81595042e-01,  3.70519305e-01,
         1.04388437e+00,  1.89082212e-01,  1.45528285e-01,
         4.23396820e-02,  1.87350431e-01,  3.79447224e-01,
        -2.44967319e-02,  1.03080524e+00,  2.45373491e-01,
         1.48500287e-01, -1.84458484e-02,  1.05747085e-01,
         1.43188913e-01, -9.21601941e-02,  1.03448044e+00,
         2.37686201e-01,  1.57910339e-01,  6.31256641e-02,
         9.98910975e-02,  3.47946092e-02, -8.29902542e-02,
         1.01686801e+00,  2.19058660e-01,  2.11874534e-01,
         6.43425038e-01,  1.22208266e+00,  1.13143979e-01,
         5.87718619e-01,  2.32728234e+00,  2.50180646e+00,
         1.67616014e+00,  1.50247946e+00,  1.15250613e-01,
         6.90922111e-01,  2.80351758e+00,  2.47027800e+00,
         2.07479072e+00,  2.19090987e+00,  2.30791503e-01,
         1.10391389e+00,  4.07483792e+00,  4.07580663e+00,
         4.20232054e+00,  9.86444833e-01,  1.53580069e-01,
         4.47778245e-01,  2.19854463e+00,  3.05042391e+00,
         1.77474574e+00,  8.13878292e-01,  3.81977412e-01,
         5.56938726e-01,  2.33798871e+00,  3.16732039e+00,
         2.98864187e+00,  1.05281505e+00,  8.89849188e-01,
         7.64094605e-01,  3.72131968e+00,  3.94289877e+00,
         4.25152635e+00,  1.63887148e+00,  2.03717508e-01,
         6.63191921e-01,  3.51381506e+00,  3.00615958e+00,
         2.53146245e+00,  1.59762297e+00,  3.20449289e-01,
         7.37424192e-01,  3.62878437e+00,  3.09787465e+00,
         3.37104332e+00,  1.82392562e+00,  5.02762965e-01,
         9.22637952e-01,  7.57546028e+00,  5.81452482e+00,
         1.01138957e+01,  9.97285514e-01,  1.25242090e-01,
         4.78930661e-01,  2.21200699e+00,  2.79841791e+00,
         1.63051142e+00,  7.10268526e-01,  2.99661889e-01,
         5.20895019e-01,  2.43127224e+00,  2.86324753e+00,
         2.40056286e+00,  7.78197995e-01,  7.35337395e-01,
         7.05945609e-01,  3.81738386e+00,  3.69359533e+00,
         3.45261913e+00,  1.73591878e+00,  1.98598571e-01,
         8.65890009e-01,  3.08243277e+00,  3.51064189e+00,
         2.70032701e+00,  1.68721444e+00,  2.79840965e-01,
         1.03399237e+00,  3.19009086e+00,  3.61226326e+00,
         3.49313400e+00,  1.91472078e+00,  4.36654245e-01,
         1.25795625e+00,  7.23399874e+00,  9.17431684e+00,
         1.31086159e+01,
         1.00000000e+00,  1.00000000e+00,  1.00000000e+00,
         1.00000000e+00,  1.00000000e+00,  1.00000000e+00,
         1.00000000e+00,  1.00000000e+00,  1.00000000e+00,
         1.00000000e+00,  1.00000000e+00,  1.00000000e+00]],
    "comment__": "abdoment                  neck,                   right_hip                 right_knee     right_ankle                right_shoulder          right_elbow, right_wrist,   left_hip                 left_knee  left_ankle,              left_shoulder            left_elbow,    right_wrist",	     
"action_bounds": [[-1.0, -1.0, -1.0, -0.5, -1.0, -1.0, -1.0, -2.57, -1.0, -1.0, -1.0, -2.57,  -3.14,        -1.0, -1.0, -1.0, -1.57,   -1.0, -1.0, -1.0, -2.57,  -3.14,       -0.1,          -1.0, -1.0, -1.0, -1.57,   -3.14,   -1.0, -1.0, -1.0, -2.57,  -1.0, -1.0, -1.0, -2.57, -3.14,        -0.1],
				  [ 1.0,  1.0,  1.0,  0.5,  1.0,  1.0,  1.0,  2.57,  1.0,  1.0,  1.0,  2.57, 0.5,            1.0,  1.0,  1.0,  1.57,    1.0,  1.0,  1.0,  2.57,   0.50,        0.1,           1.0,  1.0,  1.0,  1.57,     0.5,    1.0,  1.0,  1.0,  2.57,   1.0,  1.0,  1.0,  2.57,  0.5,          0.1]],
	"comment__": "Set of discrete actions that can be sampled from",	
	"comment__": "   grab_elbow_desired_position, grab_arm_release_angle, next_grab_time, free_elbow_angle",								  
"discrete_actions": [[-0.1, -2.3,   0.15,  -1.35, -3.0,  1.2],
		             [-2.2, -0.74,  1.25,  -0.11, -1.3,  0.7],
		             [-0.1, -0.4,   0.35,  -1.44,  0.3, -1.1],
		             [1.1, -2.45,   0.45,  -0.53, -1.43, 0.7],
		             [-0.1, -0.55,  0.35,   1.23,  0.3,  1.17],
		             [ 0.14, -0.45,-0.45,  -0.15, -1.53, 1.27],
		             [ 1.1,  -3.1,  0.55,  -0.12,  0.3, -0.7],
		             [-1.1,  -0.65,-0.35,  -0.67, -2.3,  0.7],
		             [-0.1,   0.25, 0.30,   1.71,  0.3,  1.17]], 
    "comment__": "Is action space continuous or discrete?",
"action_space_continuous":true,
    "comment__": "Should the method train on the validation set only",
"train_on_validation_set":false,
    "comment__": "Name of the type of simulator to use",
"environment_type": "terrainRLSim",
    "comment__": "Model type to use for the forward dynamics model",
"forward_dynamics_predictor": "network",
    "comment__": "Method to be used for the forward dynamics model is the model types uses a simulator",
"sampling_method": "SequentialMC",
    "comment__": "Use the action suggested by the policy to start the sampling method.",
"use_actor_policy_action_suggestion": true,
    "comment__": "If selecting from a uniform distribution the number of regularly distant samples to take / action dimension",
"num_uniform_action_samples": 3,
    "comment__": "Number of steps ahead the actions should be sampled",
"look_ahead_planning_steps": 2,
    "comment__": "How often to update the training data and plots wrt # of rounds",
"plotting_update_freq_num_rounds": 5,
    "comment__": "How often to save the training data and plotting data",
"saving_update_freq_num_rounds": 5,
    "comment__": "Number of treads that can be run in parallel during training",
"num_available_threads": 16,
    "comment__": "Length of the queues used to pass simulation data between the simulation workers and the learning agent(s).",
"queue_size_limit": 10,
    "comment__": "Number of actions performed between training updates",
"sim_action_per_training_update": 8,
    "comment__": "Number of rounds of adaptive sampling",
"adaptive_samples": 5,
    "comment__": "Number of elite adaptive samples to keep between adaptive sampling rounds",
"num_adaptive_samples_to_keep": 50,
    "comment__": "Use the variance calculated from the policy network (calculated using dropout)",
"use_actor_policy_action_variance_suggestion": false,
    "comment__": "Method used for action exploration",
"exploration_method": "gaussian_network",
    "comment__": "Amount of dropout to use in the networks (if using a dropout network)",
"dropout_p": 0.0,
    "comment__": "Regularization weight for the policy network",
"regularization_weight": 0.000001,
    "comment__": "Some parameter for rmsprop stocastic gradient optimization method.",
"rho": 0.95,
    "comment__": "Some parameter for rmsprop stocastic gradient optimization method.",
"rms_epsilon": 0.000001,
    "comment__": "Number of training updates before the target network is updated",
"steps_until_target_network_update": 100000000,
    "comment__": "Initial factor epsilon in multiplied by (This value will slowly be reduced during training)",
"epsilon_annealing": 0.8,
    "comment__": "Different ways of calculating the scaling method used normalize the input and outputs of the network from the bootstrapping samples. minmax, input and output are -mean/max-min. variance, input and output are -mean/(std*2), given, use the bounds provided in this file",
"state_normalization": "adaptive",
    "comment__": "load a pretrained model for the controller",
"load_saved_model": false,
	"comment__": "Number of updates the critic should perform per actor update",
"critic_updates_per_actor_update": 4,
    "comment__": "weather or not to clamp actions to stay inside the action boundaries",
"clamp_actions_to_stay_inside_bounds": false,
    "comment__": "Number of initial actions to sample before calculating input/output scaling and starting to train.",
"bootstrap_samples": 10000,
    "comment__": "What method to use to select actions during bootstrapping",
"bootsrap_with_discrete_policy": true,
    "comment__": "That max number of action that can be take before the end of an episode/epoch",
"max_epoch_length": 128,
    "comment__": "If reward is below this bound it will not be put in the Experience Buffer",
"reward_lower_bound": -300.5,
    "comment__": "Enable guided policy search. Uses MCMC sampling ahead in time to select the best action to keep",
"use_guided_policy_search" : false,
    "comment__": "The number of training updates to perform for every action that is simulated",
"training_updates_per_sim_action": 1,
    "comment__": "Use The forward dynamics simulator as a way of sampling suggested actions for exploration",
"use_sampling_exploration": false,
    "comment__": "Use the forward dyanmics model to perform action exploration wrt to V -> fd > delta Action gradients",
"use_model_based_action_optimization": false,
    "comment__": "Flag for policy evaluation to swap in the task network from one model and the character/robot network from another",
"use_transfer_task_network": false,
    "comment__": "Add a large cost to actions that are suggested to be outside the action bounds.",
"penalize_actions_outside_bounds": false,
    "comment__": "Network type to use for the forward dynamics model",
"forward_dynamics_model_type": "model.FDNNKerasAdaptive.FDNNKerasAdaptive",
    "comment__": "Whether or not to save the Experience memory after bootstrapping",
"save_experience_memory": false,
    "comment__": "Whether or not to train the policy and critic?",
"train_rl_learning": true,
    "comment__": "Force the character to start each new action in a good state, close to a good solution",
"use_back_on_track_forcing": false,
    "comment__": "draw/render the next state suggested by the forward dynamics model",
"visualize_forward_dynamics": false,
    "comment__": "Learning rate for the forward dynamics model",
"fd_learning_rate": 0.0001,
    "comment__": "Whether or not to train the policy. Used for debugging",
"train_actor": true,
    "comment__": "Plot the terms for the critic as well (regularization and td error)",
"debug_critic": false,
    "comment__": "critic regularization weight",
"critic_regularization_weight": 0.00001,
    "comment__": "Critic learning rate",
"critic_learning_rate": 0.0001,
    "comment__": "During evaluation plot of value function",
"visualize_expected_value": true,
    "comment__": "exponential decay value for use in reward function",
"target_velocity_decay":-2.0,
    "comment__": "Target velocity for controller",
"target_velocity":0.0,
    "comment__": "Number of terrain features for which convolutinoal filters should be used",
"num_terrain_features": 0,
    "comment__": "Initial tempurature for annealing of e-greedy exploration",
"initial_temperature": 1.0,
    "comment__": "epsilon lower limit",
"min_epsilon": 0.05,
    "comment__": "Whether or not to draw/render the simulation",
"shouldRender": true,
    "comment__": "Learning rate use for model based action exploration",
"action_learning_rate": 0.50,
    "comment__": "During model-based action exploration, Probability of a random action being generated from MBAE,",
"model_based_action_omega": 0.5,
	"comment__": "visualize the loss and regularization cost of the actor. Warning: might use a lot of memory",
"debug_actor": true,
	"comment__": "float type to use, if for example you want to train on the GPU use float32",
"float_type": "float32",
	"comment__": "What processor type to perform the training on [cpu|gpu]",
"training_processor_type": "cpu",
	"comment__": "optimizer is the type of optimization algorithm to use",
"optimizer": "adam",
	"comment__": "This setting uses a sampling method overtop of simulation",
"use_simulation_sampling": false,
	"comment__": "Variance scaling used for sampling",
"variance_scalling": 0.1,
	"comment__": "Whether or not to parameterize the control via the reward function, also adds parameters to input state",
"use_parameterized_control": false,
	"comment__": "parameter bounds for parameterized controller",
"controller_parameter_settings": {
		"velocity_bounds": [[0.5],[2.5]]
		},
	"comment__": "The parameter used to control the average change in the control parameters",
"average_parameter_change": 0.25,
	"comment__": "Whether or not to train the value function some output from the forward dynamics",
"train_critic_on_fd_output": true,
	"comment__": "Use to add an additional regularization term to prevent the network from moving to far from its previous values",
"use_previous_value_regularization": false,
	"comment__": "Controls the level of information that is printed to the terminal",
"print_level": "train",
	"comment__": "print level descriptions",
"print_levels": {
		"debug": 1,
		"train": 0,
		"hyper_train": -1,
		"testing_sim": -2
		},
	"comment__": "The type of function to apply over the controller target values [gaussian|abs]",
"reward_smoother": "gaussian",
	"comment__": "Weights for different components of the reward function",
"controller_reward_weights": {
		"velocity": 0.8,
		"torque": 0.05,
		"root_height": 0.05,
		"root_pitch": 0.1,
		"right_hand_x_pos": 0.0
		},
	"comment__": "Regularization weight for different between policy parameters and old policy parameters",
"previous_value_regularization_weight":  0.01,
	"comment__": "Random seed value for the simulation to use",
"random_seed": 1234,
	"comment__": "KL divergence threshold between policy updates",
"kl_divergence_threshold": 0.25,
	"comment__": "Makes a few changes to the flow of control in order for things to be on policy [true|fast]",
"on_policy": true,
	"comment__": "Whether or not to use a stochastic policy, This adds more outputs to the network and changes the way actions are sampled",
"use_stochastic_policy": false,
	"comment__": "Whether or  not to train the critic at all. Usually used for debugging",
"train_critic": true,
	"comment__": "What type of regularization to use",
"regularization_type": "kl",
	"comment__": "Whether or not to collects tuples in batches, this can be good for multi-threading or computing furture discounted reward",
"collect_tuples_in_batches":false,
	"comment__": "Whether or not the controller should be reset to a new epoch when a fall (fallen into some kind of non-recoverable state) has occured",
"reset_on_fall": true,
	"comment__": "Whether a model of the reward r <- R(s,a) should be trained",
"train_reward_predictor": false,
	"comment__": "How many gradient steps model based action exploration should take",
"num_mbae_steps": 1,
    "comment__": "Whether or not the actor buffer should be fixed to process batches of a certain size",
"fix_actor_batch_size": true,
	"comment__": "The number of critic updates that are done between every dyna update step.",
"dyna_update_lag_steps": 5,
	"comment__": "Use generalized advantage estimation",
"use_GAE": true,
	"comment__": "generalized advantage estimation lambda in [0,1], when =0, this is just a one step return (lots of bias), when =1, use only data no variance reduction",
"GAE_lambda": 0.95,
	"comment__": "Whether or not to take a couple of on policy updates",
"use_multiple_policy_updates": false,	
	"comment__": "Whether or not to clear the experience memory after each update is done, for pure on policy methods",
"clear_exp_mem_on_poli": true,
	"comment__": "Disable scaling for inputs and outputs of networks",
"disable_parameter_scaling": false,
	"comment__": "Train a state encoding as well.",
"train_state_encoding": false,
	"comment__": "std entropy weight to help encourage exploration",
"std_entropy_weight": 0.01,
	"comment__": "policy loss function weight, to help balance policy loss vs value function loss",
"policy_loss_weight": 1.0,
	"comment__": "number of on policy rollouts to perform per epoch",
"num_on_policy_rollouts": 8,
	"comment__": "Don't weight policy updates wrt the advantage of the action",
"dont_use_advantage": false,
	"comment__": "PPO should use seperate networks for the value and policy functions",
"use_random_actions_for_MBAE": false,
	"comment__": "Only use exploratory actions to update the policy",
"only_use_exp_actions_for_poli_updates": true,
	"comment__": "Multiply the MBAE action by a sample from a uniform distribution, to allow the action to vary in magnitude",
"randomize_MBAE_action_length": true,
	"comment__": "Load a saved and pre-trained forward dynamics model",
"load_saved_fd_model": false,
	"comment__": "Whether or not to use a stochastic forward dynamics model",
"use_stochastic_forward_dynamics": false,
	"comment__": "Normalize the advantage used for gradient calculations wrt the discount factor and scale of reward",
"normalize_advantage": true,
	"comment__": "use a special annealing schedule that will being at 1 and end at 0",
"annealing_schedule": "linear",
	"comment__": "Use the annealing schedule in on policy learning",
"anneal_on_policy": true,
	"comment__": "A list describing the size of the layers for the NN",
"critic_network_layer_sizes": [["dropout", 0.2], 512, ["dropout", 0.2], 256, ["dropout", 0.2]],
	"comment__": "A list describing the size of the layers for the NN",
"policy_network_layer_sizes": [512, 256],
	"comment__": "Activation function to use for the std part of the policy",
"_last_std_policy_layer_activation_type": "sigmoid",
	"comment__": "type of activation to use",
"activation_type": "leaky_rectify",
	"comment__": "type of activation to use for policy",
"policy_activation_type": "leaky_rectify",
	"comment__": "last policy type of activation to use",
"last_policy_layer_activation_type": "tanh",
	"comment__": "Use a different off-policy experience memory for the fd learning",
"keep_seperate_fd_exp_buffer": false,
	"comment__": "Train a GAN to do something....",
"train_gan": false,
	"comment__": "Train GAN with gaussian noise",
"train_gan_with_gaussian_noise": true,
	"comment__": "This overrides all of the envs to be a single one from the list of sim environments",
"override_sim_env_id": false,
	"comment__": "Prevents the critic from including MBAE actions in it's policy estimation.",
"give_mbae_actions_to_critic": false,
	"comment__": "The interpolation parameter used adjust the target network for DDPG",
"target_net_interp_weight": 0.001,
	"comment__": "Batch size for the value function network",
"value_function_batch_size": 64,
	"comment__": "Train the critic off of the data stored in the exp for the fd model",
"train_critic_with_fd_data": true,
	"comment__": "Additional on-policy training updates",
"additional_on-poli_trianing_updates": 1,
	"comment__": "Use average of policy std as mbae learning rate",
"use_std_avg_as_mbae_learning_rate": true,
	"comment__": "Use the on-policy value function to compute state gradients instead of off-policy one for SMBAE.",
"use_extra_value_for_MBAE_state_grads": true,
	"comment__": "Performs MBAE random sampling at the level of episodes instead of transitions",
"perform_mbae_episode_sampling": false,
	"comment__": "Using a single network for all functions",
"use_single_network": false,
	"comment__": "Anneal the std of the policy as well",
"anneal_policy_std": false,
	"comment__": "anneal the probability of performing explorative actions, set to >= 1.0 to perform only exploration actions",
"anneal_exploration": 1.0,
	"comment__": "If true the simulation will terminate after an early termination or when max time steps is reached, otherwise max_time_steps sample are collected",
"sample_single_trajectories": true,
	"comment__": "Network settings",
"network_settings": {
		"comment__": "Whether or not the network should include pooling layers between convolutions.",
	"perform_convolution_pooling": false,
		"comment__": "Change where the network splits for a single network model",
	"split_single_net_earlier": false,
		"comment__": "Use CoordConv convolutional layers.",
	"use_coordconv_layers": true
},
	"comment__": "Run some initial training steps to pretrain the critic before starting policy training",
"pretrain_critic": 0,
	"comment__": "Policy distribution change factor, if beyond this updates are not performed",
"ppo_et_factor": 1.1,
	"comment__": "higher level controller timestep",
"hlc_timestep": 1,
	"comment__": "Which backend to use for keras [theano|tensorflow]",
"learning_backend": "tensorflow",
	"comment__": "Use an L2 loss over for the value function over the G_t from rollouts (no TD loss)",
"dont_use_td_learning": false,
	"comment__": "Anneal the learning rate",
"anneal_learning_rate": false,
	"comment__": "Data ordering format for convolutions",
"image_data_format": "channels_first",
	"comment__": "Size and shape of terrain",
"terrain_shape": [1,128,128],
    "comment__": "Number of terrain features for which convolutinoal filters should be used",
"num_terrain_features": 0,
	"comment__": "specify the learning algorithm to use for the fd model",
"fd_algorithm" : "algorithm.SiameseNetwork.SiameseNetwork",
	"comment__": "A list describing the size of the layers for the NN",
"fd_network_layer_sizes": [ ["Reshape", [1, 128, 128]], [16, [10, 10], [5, 5]], ["dropout", 0.2], [32, [5, 5], [2, 2]], ["dropout", 0.2], [32, [3, 3], [1, 1]], "flatten_features", "mark_middle", 256, "merge_features", ["Dense", 128, "sigmoid"]],
	"comment__": "A list describing the size of the layers for the NN",
"reward_network_layer_sizes": [["TimeDistributedConv", [31, 1, 16384], "fd"], ["GRU", 256, 128], ["Dense", 128, "linear"]],
		"comment__": "type of activation to use",
"reward_activation_type": "leaky_rectify",
	"comment__": "type of activation to use for policy",
"fd_policy_activation_type": "leaky_rectify",
	"comment__": "last policy type of activation to use",
"last_fd_layer_activation_type": "sigmoid",
	"comment__": "The number of fd network to be done per actor update.",
"fd_updates_per_actor_update": 0.5,
	"comment__": "Lets the network models handle the training and batch processing",
"model_perform_batch_training": false,
	"comment__": "Leave off the final layer of the network that would result in a state sized output",
"fd_network_leave_off_end": true,
	"comment__": "Dropout for fd network",
"fd_network_dropout": 0.0,
	"comment__": "Use learned reward function",
"use_learned_reward_function": false,
	"comment__": "Mix different state description types, used for debugging visual imitation learning",
"use_dual_state_representations": false,
	"comment__": "Use vis state for policy instead of dense state",
"use_viz_for_policy": false,
	"comment__": "Dense state size",
"dense_state_size": 157,
	"comment__": "Size and shape of terrain",
"fd_terrain_shape": [1,128,128],
    "comment__": "Number of terrain features for which convolutinoal filters should be used",
"fd_num_terrain_features": 16384,
	"comment__": "replace the viz state in the next state with the state from the imitation agent",
"replace_next_state_with_imitation_viz_state": false,
	"comment__": "Type of smoothing funtion to filter the learned distance metric by",
"learned_reward_smoother": "gaussian",
	"comment__": "Just train the fd model in an LSTM fashion",
"train_LSTM_FD": false,
	"comment__": "Use a stateful LSTM",
"train_LSTM_FD_stateful": false,
	"comment__": "batch size for lstm fd models",
"lstm_batch_size": [4, 32],
	"comment__": "experience size for fd network",
"fd_expereince_length": 1000,
	"comment__": "A mapping from working_id to task id",
"worker_to_task_mapping": [0, 1, 2, 3, 4, 5, 6, 7, 0, 0, 0, 0, 0, 0, 0, 0],
	"comment__": "A small offset for comparing states that we now are imperfect comparisons",
"imperfect_compare_offset": 0.1,
	"comment__": "Force the simulation workers to use the CPU only for network models",
"force_sim_net_to_cpu": false,
	"comment__": "Just train the fd model in an LSTM fashion",
"train_LSTM_Reward": false,
	"comment__": "Leave off the final layer of the reward network that would result in a state sized output",
"reward_network_leave_off_end": true,
	"comment__": "Episode simulation time out",
"simulation_timeout": 1800,
	"comment__": "Include some randomly sized sequences when training lstms",
"use_random_sequence_length_for_lstm": true,
	"comment__": "Use a sparse sequence based reward",
"use_sparse_sequence_based_reward": false,
	"comment__": "Add label noise to Discriminator/Simease network training",
"add_label_noise": 0.1,
	"comment__": "Add velocity state to image state",
"append_camera_velocity_state": "3D",
	"comment__": "Add a negative reward when the robot falls",
"use_fall_reward_shaping2": false
}