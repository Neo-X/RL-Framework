{
    "comment__": "Anneal the learning rate",
    "model_type": "model.DeepNNKerasAdaptive.DeepNNKerasAdaptive",
    "agent_name": "algorithm.TD3_KERAS.TD3_KERAS",
    "data_folder": "PD_Humanoid3D_MutliChar_ScenarioSpace_5_OnlyVel_SimpleReward_v1/Tensorflow_TRUE-v0_GoodLLC_24/2021_11_10_12_55_42_698531",
    "comment": "initial probability of selecting a discrete random action",
    "epsilon": 1.0,
    "omega": 0.0,
    "batch_size": 128,
    "learning_rate": 5e-05,
    "sim_config_file": "PD_Humanoid3D_MutliChar_ScenarioSpace_5_OnlyVel_SimpleReward_v1",
    "exploration_rate": [
        0.3
    ],
    "rounds": 1000,
    "epochs": 8,
    "eval_epochs": 8,
    "discount_factor": 0.95,
    "visualize_learning": false,
    "save_trainData": true,
    "train_forward_dynamics": false,
    "reward_bounds": [
        [
            [
                0.0
            ],
            [
                1.0
            ]
        ]
    ],
    "experience_length": [
        100000
    ],
    "state_bounds": [
        "ask_env"
    ],
    "action_bounds": [
        [
            [
                -0.5,
                -1.0,
                -1.0,
                -1.5,
                -1.57
            ],
            [
                1.0,
                1.0,
                2.0,
                1.5,
                1.57
            ]
        ]
    ],
    "discrete_actions": [
        [
            -0.1,
            -2.3,
            0.15,
            -1.35,
            -3.0,
            1.2
        ],
        [
            -2.2,
            -0.74,
            1.25,
            -0.11,
            -1.3,
            0.7
        ],
        [
            -0.1,
            -0.4,
            0.35,
            -1.44,
            0.3,
            -1.1
        ],
        [
            1.1,
            -2.45,
            0.45,
            -0.53,
            -1.43,
            0.7
        ],
        [
            -0.1,
            -0.55,
            0.35,
            1.23,
            0.3,
            1.17
        ],
        [
            0.14,
            -0.45,
            -0.45,
            -0.15,
            -1.53,
            1.27
        ],
        [
            1.1,
            -3.1,
            0.55,
            -0.12,
            0.3,
            -0.7
        ],
        [
            -1.1,
            -0.65,
            -0.35,
            -0.67,
            -2.3,
            0.7
        ],
        [
            -0.1,
            0.25,
            0.3,
            1.71,
            0.3,
            1.17
        ]
    ],
    "action_space_continuous": true,
    "train_on_validation_set": true,
    "environment_type": "GymMultiChar",
    "forward_dynamics_predictor": "network",
    "sampling_method": "SequentialMC",
    "use_actor_policy_action_suggestion": true,
    "num_uniform_action_samples": 3,
    "look_ahead_planning_steps": 2,
    "plotting_update_freq_num_rounds": 2,
    "saving_update_freq_num_rounds": 5,
    "num_available_threads": 16,
    "queue_size_limit": 200,
    "sim_action_per_training_update": 1,
    "adaptive_samples": 5,
    "num_adaptive_samples_to_keep": 50,
    "use_actor_policy_action_variance_suggestion": false,
    "exploration_method": "gaussian_random",
    "dropout_p": 0.0,
    "regularization_weight": 1e-05,
    "rho": 0.95,
    "rms_epsilon": 1e-06,
    "steps_until_target_network_update": 50000000,
    "epsilon_annealing": 0.8,
    "state_normalization": "adaptive",
    "load_saved_model": false,
    "critic_updates_per_actor_update": 4,
    "clamp_actions_to_stay_inside_bounds": false,
    "bootstrap_samples": 1,
    "bootsrap_with_discrete_policy": true,
    "max_epoch_length": 128,
    "reward_lower_bound": -10.5,
    "use_guided_policy_search": false,
    "training_updates_per_sim_action": 1,
    "use_sampling_exploration": false,
    "use_model_based_action_optimization": true,
    "use_transfer_task_network": false,
    "penalize_actions_outside_bounds": false,
    "forward_dynamics_model_type": "model.ForwardDynamicsDenseNetworkDropoutTesting.ForwardDynamicsDenseNetworkDropoutTesting",
    "save_experience_memory": "continual",
    "train_rl_learning": true,
    "use_back_on_track_forcing": false,
    "visualize_forward_dynamics": false,
    "fd_learning_rate": 0.001,
    "train_actor": true,
    "debug_critic": false,
    "critic_regularization_weight": 1e-05,
    "critic_learning_rate": 0.0002,
    "visualize_expected_value": true,
    "target_velocity_decay": -2.0,
    "target_velocity": 0.0,
    "num_terrain_features": 2048,
    "initial_temperature": 10.0,
    "min_epsilon": 0.35,
    "shouldRender": false,
    "action_learning_rate": 0.5,
    "model_based_action_omega": 0.2,
    "debug_actor": false,
    "float_type": "float32",
    "training_processor_type": "cpu",
    "optimizer": "adam",
    "use_simulation_sampling": false,
    "variance_scalling": 0.1,
    "use_parameterized_control": false,
    "controller_parameter_settings": {
        "velocity_bounds": [
            [
                0.5
            ],
            [
                2.5
            ]
        ]
    },
    "average_parameter_change": 0.25,
    "train_critic_with_fd_data": false,
    "use_previous_value_regularization": false,
    "print_level": "train",
    "print_levels": {
        "debug": 1,
        "train": 0,
        "hyper_train": -1,
        "testing_sim": -2
    },
    "reward_smoother": "gaussian",
    "controller_reward_weights": {
        "velocity": 0.8,
        "torque": 0.05,
        "root_height": 0.05,
        "root_pitch": 0.1,
        "right_hand_x_pos": 0.0
    },
    "previous_value_regularization_weight": 0.01,
    "random_seed": 0,
    "kl_divergence_threshold": 0.1,
    "on_policy": true,
    "use_stochastic_policy": false,
    "train_critic": true,
    "regularization_type": "kl",
    "collect_tuples_in_batches": false,
    "reset_on_fall": true,
    "train_reward_predictor": true,
    "num_mbae_steps": 1,
    "fix_actor_batch_size": true,
    "anneal_mbae": true,
    "dyna_update_lag_steps": 5,
    "use_GAE": true,
    "GAE_lambda": 0.95,
    "use_multiple_policy_updates": false,
    "clear_exp_mem_on_poli": false,
    "disable_parameter_scaling": false,
    "train_state_encoding": false,
    "std_entropy_weight": 0.01,
    "policy_loss_weight": 1.0,
    "num_on_policy_rollouts": 8,
    "dont_use_advantage": false,
    "use_random_actions_for_MBAE": false,
    "only_use_exp_actions_for_poli_updates": true,
    "randomize_MBAE_action_length": true,
    "load_saved_fd_model": false,
    "network_description_type": "json",
    "critic_network_layer_sizes": [
        [
            {
                "layer_type": "slice",
                "slice_index": 2244,
                "slice_label": "HLP_goal"
            },
            {
                "layer_type": "slice",
                "slice_index": 2048,
                "slice_label": "agent_propreceptive0"
            },
            {
                "layer_type": "Reshape",
                "target_shape": [
                    2,
                    32,
                    32
                ]
            },
            {
                "layer_type": "conv2d",
                "filters": 8,
                "kernel_size": [
                    6,
                    6
                ],
                "strides": [
                    3,
                    3
                ],
                "use_bias": true
            },
            {
                "layer_type": "activation",
                "activation_type": "leaky_rectify"
            },
            {
                "layer_type": "conv2d",
                "filters": 16,
                "kernel_size": [
                    4,
                    4
                ],
                "strides": [
                    2,
                    2
                ],
                "use_bias": true
            },
            {
                "layer_type": "Flatten"
            },
            {
                "layer_type": "Concatenate",
                "slice_label": "agent_propreceptive0"
            },
            {
                "layer_type": "Concatenate",
                "slice_label": "HLP_goal"
            },
            {
                "layer_type": "integrate_actor_part"
            },
            {
                "layer_type": "activation",
                "activation_type": "leaky_rectify"
            },
            {
                "layer_type": "Dense",
                "units": 256,
                "use_bias": true
            },
            {
                "layer_type": "Concatenate",
                "slice_label": "HLP_goal"
            },
            {
                "layer_type": "integrate_actor_part"
            },
            {
                "layer_type": "activation",
                "activation_type": "leaky_rectify"
            },
            {
                "layer_type": "Dense",
                "units": 128,
                "use_bias": true
            },
            {
                "layer_type": "Concatenate",
                "slice_label": "HLP_goal"
            },
            {
                "layer_type": "integrate_actor_part"
            },
            {
                "layer_type": "activation",
                "activation_type": "leaky_rectify"
            }
        ]
    ],
    "policy_network_layer_sizes": [
        [
            {
                "layer_type": "slice",
                "slice_index": 2244,
                "slice_label": "HLP_goal"
            },
            {
                "layer_type": "slice",
                "slice_index": 2048,
                "slice_label": "agent_propreceptive0"
            },
            {
                "layer_type": "Reshape",
                "target_shape": [
                    2,
                    32,
                    32
                ]
            },
            {
                "layer_type": "conv2d",
                "filters": 8,
                "kernel_size": [
                    6,
                    6
                ],
                "strides": [
                    3,
                    3
                ],
                "use_bias": true
            },
            {
                "layer_type": "activation",
                "activation_type": "leaky_rectify"
            },
            {
                "layer_type": "conv2d",
                "filters": 16,
                "kernel_size": [
                    4,
                    4
                ],
                "strides": [
                    2,
                    2
                ],
                "use_bias": true
            },
            {
                "layer_type": "Flatten"
            },
            {
                "layer_type": "Concatenate",
                "slice_label": "agent_propreceptive0"
            },
            {
                "layer_type": "Concatenate",
                "slice_label": "HLP_goal"
            },
            {
                "layer_type": "activation",
                "activation_type": "leaky_rectify"
            },
            {
                "layer_type": "Dense",
                "units": 256,
                "use_bias": true
            },
            {
                "layer_type": "Concatenate",
                "slice_label": "HLP_goal"
            },
            {
                "layer_type": "activation",
                "activation_type": "leaky_rectify"
            },
            {
                "layer_type": "Dense",
                "units": 128,
                "use_bias": true
            },
            {
                "layer_type": "Concatenate",
                "slice_label": "HLP_goal"
            },
            {
                "layer_type": "activation",
                "activation_type": "leaky_rectify"
            }
        ]
    ],
    "_last_std_policy_layer_activation_type": "sigmoid",
    "activation_type": "leaky_rectify",
    "last_policy_layer_activation_type": "tanh",
    "policy_activation_type": "leaky_rectify",
    "normalize_advantage": true,
    "annealing_schedule": "log",
    "anneal_on_policy": true,
    "keep_seperate_fd_exp_buffer": false,
    "train_gan": false,
    "train_gan_with_gaussian_noise": true,
    "override_sim_env_id": false,
    "give_mbae_actions_to_critic": true,
    "train_extra_value_function": true,
    "target_net_interp_weight": 0.001,
    "value_function_batch_size": 32,
    "use_std_avg_as_mbae_learning_rate": true,
    "use_extra_value_for_MBAE_state_grads": true,
    "CACLA_use_advantage": false,
    "perform_mbae_episode_sampling": false,
    "additional_on_policy_training_updates": 4,
    "anneal_policy_std": false,
    "terrain_shape": [
        2,
        32,
        32
    ],
    "image_data_format": "channels_first",
    "llc_policy_model_path": "/home/ruizhang/Desktop/tmp/doodad-output/terrainRLSim/algorithm.PPO_KERAS.PPO_KERAS/PD_Humanoid3D_LLC_Symetric_Step_MultiAgent_Terrain_WithObs_v1/Tensorflow_Pushes_15/2021_08_18_00_34_54_265491/0/model.DeepNNKerasAdaptive.DeepNNKerasAdaptive/Humanoid_Flat_Tensorflow_MultiAgent_WithObs_LLC_v3.json",
    "hlc_timestep": 15,
    "anneal_exploration": false,
    "sample_single_trajectories": true,
    "network_settings": {
        "comment__": "Change where the network splits for a single network model",
        "perform_convolution_pooling": false,
        "split_single_net_earlier": false
    },
    "pretrain_critic": 0,
    "ppo_et_factor": 1.2,
    "learning_backend": "tensorflow",
    "model_perform_batch_training": false,
    "anneal_learning_rate": false,
    "experiment_logging": {
        "use_comet": true,
        "project_name": "hlc"
    }
}