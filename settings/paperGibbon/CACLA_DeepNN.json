{
"model_type": "Deep_NN",
"agent_name": "CACLA",
"data_folder": "Good_Anchors/",
"epsilon": 0.45, 
"omega": 0.75,
"batch_size": 32,
"learning_rate": 0.0002,
"anchor_file": "../data/anchorData/paperGibbonAnchors.json",
"sim_config_file": "../data/paperGibbon/trainContinuousActions.ini",
"forwardDynamics_config_file": "../data/paperGibbon/trainContinuousActions.ini",
"num_actions": 9,
"exploration_rate": 0.08,
"rounds": 5000,
"epochs": 10,
"eval_epochs": 10,
"num_states": 32,
"discount_factor": 0.98,
"visualize_learning": true,
"save_trainData": true,
"train_forward_dynamics": false,
"visulaize_forward_dynamics": false,
"reward_bounds": [[-10.1],[0.0]],
"expereince_length": 10000,
"state_bounds": [[-3.14, 1.0,-8.05, -1.0, -1.0, -8.5, -5.0, -5.5, -5.0, -2.5, -5.0],
				 [ 3.14, 8.5, 5.00, 1.0, 1.0, 10.0, 5.0, 15.0,  5.0, 20.0, 5.0]],
"action_bounds": [[-0.9,-0.3, 0.15, -0.9],
				  [-0.05, -1.1, 0.55, -0.01]],
"discrete_actions": [[-0.6, -0.3, 0.15, -0.35],
		             [-0.6, -0.74, 0.25, -0.11],
		             [-0.6, -0.4, 0.35, -0.44],
		             [-0.8, -0.45, 0.45, -0.53],
		             [-0.8, -0.55, 0.35, -0.23],
		             [-0.8, -0.45, 0.45, -0.15],
		             [-0.2, -0.5, 0.55, -0.12],
		             [-0.2, -0.65, 0.35,  -0.67],
		             [-0.2, -0.5, 0.30, -0.71]], 		  
"action_space_continuous":true,
"train_on_validation_set":true,
"environment_type": "paperGibbon",
"forward_dynamics_predictor": "network",
"sampling_method": "SequentialMC",
"use_actor_policy_action_suggestion": true,
"num_uniform_action_samples": 3,
"look_ahead_planning_steps": 2,
"plotting_update_freq_num_rounds": 5,
"saving_update_freq_num_rounds": 5,
"num_available_threads": 3,
"queue_size_limit": 100,
"sim_action_per_training_update":16,
"adaptive_samples": 5,
"num_adaptive_samples_to_keep": 50,
"use_actor_policy_action_variance_suggestion": false,
"exploration_method": "uniform_random",
"dropout_p": 0.1,
"regularization_weight": 0.000001,
"rho": 0.95,
"rms_epsilon": 0.001,
"steps_until_target_network_update": 100,
"epsilon_annealing": 0.8,
"state_normalization": "variance",
"load_saved_model": false,
"critic_updates_per_actor_update": 5,
"clamp_actions_to_stay_inside_bounds": true,
"bootsrap_samples": 300,
"bootsrap_with_discrete_policy": true,
"max_epoch_length": 32,
"reward_lower_bound": -8.0,
"use_guided_policy_search" : false,
"training_updates_per_sim_action": 1,
"use_sampling_exploration": false,
"use_model_based_action_optimization": false,
"use_transfer_task_network": false,
"penalize_actions_outside_bounds": false,
"use_transfer_task_network": false,
"forward_dynamics_model_type": "Deep_NN",
"save_experience_memory": false,
"train_rl_learning": true,
"use_back_on_track_forcing": false,
"visualize_forward_dynamics": false,
"fd_learning_rate": 0.01
}